{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "656ecdf2-1ed8-411e-80e6-9e7bf216aedc",
   "metadata": {},
   "source": [
    "## Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0ee02d-64af-4955-b591-6d7f932ecc4f",
   "metadata": {},
   "source": [
    "We dealt with the following data:\n",
    "\n",
    "- `stop_times` (timetable data)\n",
    "- `calendar` (timetable data)\n",
    "- `routes` (timetable data)\n",
    "- `trips` (timetable data)\n",
    "- `stops` (timetable data)\n",
    "- `actual_condition` (real sbb data)\n",
    "\n",
    "\n",
    "They will be loaded down there. View data structure with `dataName.printSchema()`, to view few (e.g. 5) rows, use `dataName.show(5)`\n",
    "\n",
    "timetable data are on purpose only read from data recorded at 2022/06/01, recall that:\n",
    ">The timetables are updated weekly. It is ok to assume that the weekly changes are small, and a timetable for\n",
    "a given week is thus the same for the full year - use the schedule of the most recent week for the day of the trip.\n",
    "\n",
    "It is also way too expensive to load all data (in fact I tried, the session keeps crushing)\n",
    "\n",
    "---\n",
    "\n",
    "**Proprocessed data:**\n",
    "- `stops_in_15`: Filtered out all stops outside of 15km range; \n",
    "- `walk_map`: Calculated the walking time between walkable stops, date-independent\n",
    "- `weekday_trans`: Combining all timetable data and filtering out weekends, non-business hours; (within 15km range)\n",
    "- `trans_map`: Time that takes from one stop to another for a trip (date-dependent), similar to `walk_map`\n",
    "\n",
    "---\n",
    "\n",
    "**UI:**\n",
    "- Doesn't support fuzzy search, must use exact stop names;\n",
    "- Arrive time input format HH:MM, no spaces.\n",
    "\n",
    "---\n",
    "\n",
    "**Graph:**\n",
    "Directed graph, node is `stop_id`; edge takes two values:\n",
    "- `time`: in seconds, the time it takes from one node to another;\n",
    "- `trip_id`: the id of trip, if is walk, `trip_id` = 'walk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a0e7e92-5438-4fff-9404-bf424641426d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.9/site-packages (3.1)\n",
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.9/site-packages (12.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.9/site-packages (from pyarrow) (1.23.5)\n",
      "Requirement already satisfied: fastparquet in /opt/conda/lib/python3.9/site-packages (2023.4.0)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /opt/conda/lib/python3.9/site-packages (from fastparquet) (1.5.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from fastparquet) (21.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.9/site-packages (from fastparquet) (2023.5.0)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /opt/conda/lib/python3.9/site-packages (from fastparquet) (1.23.5)\n",
      "Requirement already satisfied: cramjam>=2.3 in /opt/conda/lib/python3.9/site-packages (from fastparquet) (2.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas>=1.5.0->fastparquet) (2022.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->fastparquet) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas>=1.5.0->fastparquet) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "#Installing dependencies\n",
    "!pip install networkx\n",
    "!pip install pyarrow\n",
    "!pip install fastparquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b665567f-1764-48b1-82fc-d4aa56344353",
   "metadata": {},
   "source": [
    "## Spark stuff\n",
    "\n",
    "**In peak hours the sesson might not be able to start** and throw:\n",
    "```\n",
    "The code failed because of a fatal error:\n",
    "\tSession xxxx did not start up in 60 seconds..\n",
    "\n",
    "Some things to try:\n",
    "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
    "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
    "c) Restart the kernel.\n",
    "```\n",
    "Solution is to keep retrying ;) good luck for that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53caff7c-0de2-4e15-8fb8-c2340bbe98df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'name': 'haolli-final-project', 'executorMemory': '4G', 'executorCores': 4, 'numExecutors': 10, 'conf': {'spark.jars.repositories': 'https://repos.spark-packages.org'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Error sending http request and maximum retry encountered.\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "import os\n",
    "import json\n",
    "from IPython import get_ipython\n",
    "\n",
    "username = os.environ['RENKU_USERNAME']\n",
    "\n",
    "configuration = dict(\n",
    "    name = f\"{username}-final-project\",\n",
    "    executorMemory = \"4G\",\n",
    "    executorCores = 4,\n",
    "    numExecutors = 10,\n",
    "    conf = {\n",
    "        \"spark.jars.repositories\": \"https://repos.spark-packages.org\",\n",
    "    }\n",
    ")\n",
    "\n",
    "get_ipython().run_cell_magic('configure', line=\"-f\", \n",
    "                             cell=json.dumps(configuration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7dd6dd8-6264-4849-bfab-89ede4b64a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tInvalid status code '400' from http://iccluster044.iccluster.epfl.ch:8998/sessions with error payload: {\"msg\":\"Duplicate session name: Some(haolli-final-project) for session 7618\"}.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "%spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "440d7004-dce0-42df-ac6f-ba31a2020e79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully passed 'username' as 'username' to Spark kernel"
     ]
    }
   ],
   "source": [
    "%%send_to_spark -i username -t str -n username"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5301e5e7-06c7-403a-b5b4-ff05fc629d2f",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4a9e47-7f2d-4e35-8e49-72198334ca9b",
   "metadata": {},
   "source": [
    "### data reading\n",
    "\n",
    "We use data one week of the final presentation but in the year of 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0606136-cdb8-4d34-951f-c995f888380a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "794fa284329546ca8c0ec992704fbd6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we assume little weekly change in the timetable data (see README)\n",
    "# we wish to calculate the connections at the date of 08/06/2023 (day of oral defense)\n",
    "# for the data, there are 5 entries every month. For example, `hdfs dfs -ls /data/sbb/part_orc/timetables/calendar/year=2022/month=6`\n",
    "# will return 5 entries: day =1, 8, 15, 22, 29, each entry corresspond to the according week's data.\n",
    "# For the sake of simplicity we only choose the year of 2022.\n",
    "# Therefore we read data from 2022/6/8\n",
    "orc_file_path = \"/data/sbb/part_orc/timetables\"\n",
    "stop_times = spark.read.orc(orc_file_path + \"/stop_times/year=2022/month=6/day=8\")\n",
    "calendar = spark.read.orc(orc_file_path + \"/calendar/year=2022/month=6/day=8\")\n",
    "routes = spark.read.orc(orc_file_path + \"/routes/year=2022/month=6/day=8\")\n",
    "trips = spark.read.orc(orc_file_path + \"/trips/year=2022/month=6/day=8\")\n",
    "csv_file_path = \"/data/sbb/part_csv/timetables\"\n",
    "stops_csv = spark.read.csv(csv_file_path + \"/stops/year=2022/month=06/day=08\", header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24bddff2-3335-41ea-966b-34201948c012",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date_of_trip: string (nullable = true)\n",
      " |-- Trip_id: string (nullable = true)\n",
      " |-- Operator_id: string (nullable = true)\n",
      " |-- Operator_abk: string (nullable = true)\n",
      " |-- Operator_name: string (nullable = true)\n",
      " |-- Transport_type: string (nullable = true)\n",
      " |-- Train_number(train): string (nullable = true)\n",
      " |-- Service type(train): string (nullable = true)\n",
      " |-- Circulation_id: string (nullable = true)\n",
      " |-- Means_of_transport_text: string (nullable = true)\n",
      " |-- If_additional: string (nullable = true)\n",
      " |-- If_failed: string (nullable = true)\n",
      " |-- Stop_id: string (nullable = true)\n",
      " |-- Stop_name: string (nullable = true)\n",
      " |-- Arrival_time: string (nullable = true)\n",
      " |-- Actual_arrival_time: string (nullable = true)\n",
      " |-- an_prognose_status: string (nullable = true)\n",
      " |-- Departure_time: string (nullable = true)\n",
      " |-- Actual_departure_time: string (nullable = true)\n",
      " |-- ab_prognose_status: string (nullable = true)\n",
      " |-- Not_stop: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "actual_temp = spark.read.load(\"/data/sbb/part_orc/istdaten\", format=\"orc\", sep=\";\", inferSchema=\"true\", header=\"true\")\n",
    "actual_condition = actual_temp.withColumnRenamed(\"betriebstag\", \"Date_of_trip\")\\\n",
    "                .withColumnRenamed(\"fahrt_bezeichner\", \"Trip_id\")\\\n",
    "                .withColumnRenamed(\"betreiber_id\", \"Operator_id\")\\\n",
    "                .withColumnRenamed(\"betreiber_abk\", \"Operator_abk\")\\\n",
    "                .withColumnRenamed(\"betreiber_name\", \"Operator_name\")\\\n",
    "                .withColumnRenamed(\"produkt_id\", \"Transport_type\")\\\n",
    "                .withColumnRenamed(\"linien_id\", \"Train_number(train)\")\\\n",
    "                .withColumnRenamed(\"linien_text\", \"Service type(train)\")\\\n",
    "                .withColumnRenamed(\"umlauf_id\", \"Circulation_id\")\\\n",
    "                .withColumnRenamed(\"verkehrsmittel_text\", \"Means_of_transport_text\")\\\n",
    "                .withColumnRenamed(\"zusatzfahrt_tf\", \"If_additional\")\\\n",
    "                .withColumnRenamed(\"faellt_aus_tf\", \"If_failed\")\\\n",
    "                .withColumnRenamed(\"bpuic\", \"Stop_id\")\\\n",
    "                .withColumnRenamed(\"haltestellen_name\", \"Stop_name\")\\\n",
    "                .withColumnRenamed(\"ankunftszeit\", \"Arrival_time\")\\\n",
    "                .withColumnRenamed(\"an_prognose\", \"Actual_arrival_time\")\\\n",
    "                .withColumnRenamed(\"abfahrtszeit\", \"Departure_time\")\\\n",
    "                .withColumnRenamed(\"ab_prognose\", \"Actual_departure_time\")\\\n",
    "                .withColumnRenamed(\"durchfahrt_tf\", \"Not_stop\")\n",
    "actual_condition.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324f53aa-5aa3-4225-bc25-0486de28d812",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3e62a7-089b-4153-8f49-d1e300148fac",
   "metadata": {},
   "source": [
    "We filter out all stations that are 15kms away from the given Zurich location.\n",
    "\n",
    "For distance calculation, refer to [Haversine formula](https://en.wikipedia.org/wiki/Haversine_formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10ba279d-7026-407e-bad4-eff22b312dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+----------------+----------------+-------------+--------------+\n",
      "|      stop_id|           stop_name|        stop_lat|        stop_lon|location_type|parent_station|\n",
      "+-------------+--------------------+----------------+----------------+-------------+--------------+\n",
      "|      8500926|Oetwil a.d.L., Sc...|47.4236270123012| 8.4031825286317|         null|          null|\n",
      "|      8502186|Dietikon Stoffelbach|47.3933267759652|8.39896044679575|         null| Parent8502186|\n",
      "|8502186:0:1/2|Dietikon Stoffelbach|47.3933997509195|8.39894248049007|         null| Parent8502186|\n",
      "|      8502187|Rudolfstetten Hof...|47.3646702178563|8.37695172233176|         null| Parent8502187|\n",
      "|8502187:0:1/2|Rudolfstetten Hof...|47.3647371479356|8.37703257070734|         null| Parent8502187|\n",
      "+-------------+--------------------+----------------+----------------+-------------+--------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "from math import sin, cos, sqrt, atan2, radians\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "@F.udf(returnType=FloatType())\n",
    "\n",
    "# Filtering out all stops outside of 15km range with the Haversine formula\n",
    "def distance_calculation(latitude_1, longitude_1, latitude_2, longitude_2):\n",
    "    #Use Haversine formula. \n",
    "    #The Haversine formula calculates the distance between two points on a sphere \n",
    "    #(such as the Earth) based on their latitude and longitude.\n",
    "    radius_of_Earth = 6371.0 #Earth radius, just refer to the actual data \n",
    "    \n",
    "    #First, Convert latitude and longitude from degrees to radians\n",
    "    latitude_1 = radians(float(latitude_1))\n",
    "    latitude_2 = radians(float(latitude_2))\n",
    "    longitude_1 = radians(float(longitude_1))\n",
    "    longitude_2 = radians(float(longitude_2))\n",
    "\n",
    "    ## Haversine formula implementation\n",
    "    delta_latitude = latitude_2 - latitude_1\n",
    "    delta_longitude = longitude_2 - longitude_1\n",
    "\n",
    "    a = cos(latitude_1)*cos(latitude_2)*sin(delta_longitude/2)**2+sin(delta_latitude/2)**2\n",
    "    c = 2*atan2(sqrt(a), sqrt(1-a))\n",
    "\n",
    "    distance = radius_of_Earth * c \n",
    "    return distance\n",
    "\n",
    "stops_in_15 = stops_csv.where(distance_calculation(F.lit(47.378177), F.lit(8.540192), F.col(\"stop_lat\"), F.col(\"stop_lon\")) <=15)\n",
    "\n",
    "stops_in_15.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1cc168-a81b-466f-a32a-df10874cb970",
   "metadata": {},
   "source": [
    "With the within-15km stops, we want to preprocess the walking time, for this there are two steps:\n",
    "- Filter out stations that are too far away for walking (>500m)\n",
    "- Calculate walking time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7045bb6f-d6f4-422c-80fc-6014e19c2618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# rename the stops_in_15 dataframe\n",
    "# use crossJoin to make pairs between different stops\n",
    "# calculate the distance between a pair of stops\n",
    "# filtering out stop pairs that are too far away\n",
    "walking_df = stops_in_15.select(F.col(\"stop_id\").alias(\"stop_id_1\"), F.col(\"stop_name\").alias(\"stop_name_1\"), F.col(\"stop_lat\").alias(\"stop_lat_1\"), F.col(\"stop_lon\").alias(\"stop_lon_1\")) \\\n",
    "    .crossJoin(stops_in_15.select(F.col(\"stop_id\").alias(\"stop_id_2\"), F.col(\"stop_name\").alias(\"stop_name_2\"), F.col(\"stop_lat\").alias(\"stop_lat_2\"),F.col(\"stop_lon\").alias(\"stop_lon_2\"))) \\\n",
    "    .withColumn(\"distance\", distance_calculation(F.col(\"stop_lat_1\"), F.col(\"stop_lon_1\"), F.col(\"stop_lat_2\"), F.col(\"stop_lon_2\"))) \\\n",
    "    .select(F.col(\"stop_id_1\"), F.col(\"stop_name_1\"), F.col(\"stop_id_2\"), F.col(\"stop_name_2\"), F.col(\"distance\")) \\\n",
    "    .filter(\"distance<=0.5 and distance>0.0\")\n",
    "# calculating the time spent by walking between the filtered stop pairs\n",
    "walking_df = walking_df.withColumn(\"used_time\", walking_df.distance*1200).select(\"stop_id_1\",\"stop_name_1\",\"stop_id_2\",\"stop_name_2\",\"used_time\")\n",
    "# generating the walk map from the previous result, fill missing values with given default values.\n",
    "walk_map = walking_df.withColumn('trip_id',F.lit('walk')).withColumn('stops_id1_dep',F.lit('null')).withColumn('stops_id2_arr',F.lit('null'))\\\n",
    "                        .withColumn('route_desc', F.lit('walk')).select('trip_id','stop_id_1','stop_id_2','used_time','stops_id1_dep','stops_id2_arr','route_desc').withColumn('route_id', F.lit('walk'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24047ff5-adb1-4503-946c-251ab99d5b04",
   "metadata": {},
   "source": [
    "According to the requirement of the task, we select only the weekdays.\n",
    "\n",
    "The weekdays filtering can be done in calendar and then we use service_id to join other dataframes to get the transportation methods in weekdays.\n",
    "\n",
    "We then implement all the joins using primary keys, and filte out not-business times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3607543-d928-4a5f-b50c-00a617e6a8d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# filtering out week-end data\n",
    "weekdays_mask = calendar.where(\"monday = TRUE and tuesday = TRUE and wednesday = TRUE and thursday = TRUE and friday = TRUE\").select('service_id')\n",
    "weekday_trips = trips.join(weekdays_mask, \"service_id\")\n",
    "# joining all the data together (stops within 15km range)\n",
    "weekday_trips_routes = weekday_trips.join(routes, \"route_id\")\n",
    "weekday_stop_times = stop_times.join(weekday_trips_routes, \"trip_id\")\n",
    "weekday_trans = weekday_stop_times.join(stops_in_15, \"stop_id\")\n",
    "# filtering out non-business hours\n",
    "time_range = (8,18)\n",
    "# selecting only the columns of interest\n",
    "weekday_trans = weekday_trans.filter(F.hour(weekday_trans.arrival_time)>=time_range[0])\\\n",
    "                        .filter(F.hour(weekday_trans.departure_time)>=time_range[0])\\\n",
    "                        .filter(F.hour(weekday_trans.arrival_time)<=time_range[1])\\\n",
    "                        .filter(F.hour(weekday_trans.departure_time)<=time_range[1])\\\n",
    "                        .select(\"trip_id\",\"stop_id\",\"stop_name\",\"arrival_time\",\"departure_time\",\"stop_sequence\", \"route_desc\", \"route_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0507c550-40d0-4ac4-ad92-796139ac01bc",
   "metadata": {},
   "source": [
    "## Graph building\n",
    "\n",
    "Building the edges (time spend between two stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eaf170-7fa0-49ab-8abd-dee9f2241ed4",
   "metadata": {},
   "source": [
    "For one trip (identified by `trip_id`), we aggregate info about it and store it all in one row (per `trip_id`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "24999e60-fa27-4e2c-865a-f956d8e89af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import ArrayType, StringType, StructType, StructField, BooleanType\n",
    "\n",
    "@F.udf(returnType=ArrayType(StringType()))\n",
    "def to_line(column):\n",
    "    set_names = column.split(';')\n",
    "    line_set = []\n",
    "    for i in range(len(set_names) - 1):\n",
    "        line_set.append([set_names[i], set_names[i+1]])\n",
    "    return line_set\n",
    "\n",
    "@F.udf(returnType=ArrayType(StringType()))\n",
    "def to_timetable(arr, dep):\n",
    "    arr_time = arr.split(';')\n",
    "    dep_time = dep.split(';')\n",
    "    line_set = []\n",
    "    for i in range(len(arr_time) - 1):\n",
    "        line_set.append([dep_time[i], arr_time[i+1]])\n",
    "    return line_set\n",
    "\n",
    "@F.udf(returnType=ArrayType(StringType()))\n",
    "def to_transtype(route_desc):\n",
    "    routes_desc = route_desc.split(';')\n",
    "    routes = []\n",
    "    for i in range(len(routes_desc)):\n",
    "        routes.append(routes_desc[i])\n",
    "    return routes\n",
    "\n",
    "@F.udf(returnType=ArrayType(StringType()))\n",
    "def to_routeid(route_id):\n",
    "    routes_id = route_id.split(';')\n",
    "    routes_id = []\n",
    "    for i in range(len(routes_id)):\n",
    "        routes_id.append(routes_id[i])\n",
    "    return routes_id\n",
    "    \n",
    "\n",
    "@F.udf(returnType=ArrayType(FloatType()))\n",
    "def calculate_time(arr, dep):\n",
    "    arr_time = arr.split(';')\n",
    "    dep_time = dep.split(';')\n",
    "    time_set = []\n",
    "    for i in range(len(arr_time) - 1):\n",
    "        time = (datetime.strptime(arr_time[i+1], '%H:%M:%S') - datetime.strptime(dep_time[i], '%H:%M:%S')).total_seconds()\n",
    "        time_set.append(time)\n",
    "    return time_set\n",
    "\n",
    "@F.udf(returnType=StringType())\n",
    "def remove_parentheses(cols):\n",
    "    return cols[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4cc8b6d6-e46b-434d-8801-6ad3dc8f21e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import types as T\n",
    "\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"stops_id_group\", T.StringType()),\n",
    "    T.StructField(\"stops_name_group\", T.StringType()),\n",
    "    T.StructField(\"used_time\", T.StringType()),\n",
    "    T.StructField(\"stops_time_group\", T.StringType()),\n",
    "    T.StructField(\"trans_type\", T.StringType()),\n",
    "    T.StructField(\"route_id\", T.StringType())\n",
    "])\n",
    "\n",
    "def zip_arrays(stops_id_group, stops_name_group, used_time, stops_time_group, trans_type,route_id):\n",
    "    return list(zip(stops_id_group, stops_name_group, used_time, stops_time_group, trans_type, route_id))\n",
    "\n",
    "combine = F.udf(zip_arrays, returnType=T.ArrayType(schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7ec47b-63e5-47b7-8010-f7337838ddf0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad9831fc1424a7cbe31dad1d008d7b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for one trip, there are multiple stops, they are ordered by the `stop_sequence`\n",
    "# from the weekday_trans data, we first concatenate all data that belongs to one trip with the reduceByKey method,\n",
    "# then within one trip_id, sort by the `stop_sequence`,\n",
    "# then concatenate each attribute within one trip, separated by ';', and make a new columns for each attribute with the map method\n",
    "# finally, name the attributes.\n",
    "reduced_rdd = weekday_trans.rdd.map(\n",
    "    lambda row: (\n",
    "        row[0],\n",
    "        [(row[1], row[2], row[3], row[4], row[5], row[6],row[7])]\n",
    "    )\n",
    ").reduceByKey(\n",
    "    lambda x, y: x + y\n",
    ").map(\n",
    "    lambda row: (\n",
    "        row[0],\n",
    "        sorted(row[1], key=lambda text: int(text[4]))\n",
    "    )\n",
    ").map(\n",
    "    lambda row: (\n",
    "        row[0],\n",
    "        \";\".join([e[0] for e in row[1]]),\n",
    "        \";\".join([e[1] for e in row[1]]),\n",
    "        \";\".join([e[2] for e in row[1]]),\n",
    "        \";\".join([e[3] for e in row[1]]),\n",
    "        \";\".join([e[4] for e in row[1]]),\n",
    "        \";\".join([e[5] for e in row[1]]),\n",
    "        \";\".join([e[6] for e in row[1]]),\n",
    "    )\n",
    ")\n",
    "\n",
    "reduced_df = reduced_rdd.toDF(\n",
    "    [\"trip_id\", \"stop_id\", \"stop_name\", \"arrival_time\", \"departure_time\", \"stop_sequence\", \"route_desc\",\"route_id\"]\n",
    ")\n",
    "# call reduced_df.show(5) to get a feeling of what's there.\n",
    "\n",
    "# for the reduced_df, we have:\n",
    "# trip_id: one single string referring to a trip, the trip is time-dependent\n",
    "# stop_id: a series of string, split by ';', the sequential stop_id along the trip, sorted by the stop_sequence.\n",
    "# stop_name: a series of string, split by ';', the sequential stop_name along the trip, sorted by the stop_sequence.\n",
    "# arrival_time: arrival time at each stop, separated by ';'\n",
    "# departure_time: departure time at each stop, separated by ';'\n",
    "# stop_sequence: the stop sequence, not useful, will be discarded after\n",
    "# route_desc: the same values separated by ';'\n",
    "# route_id: the same route_ids, separated by ';'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5edf8090-d25e-4053-80c5-88808847decf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merge2stops_df = reduced_df.withColumn(\n",
    "    'stops_id_group', to_line(reduced_df.stop_id) \n",
    "    # to_line allows a length-2 window sliding across the stop_id sequence, to make pairs of stops along the trip\n",
    "    # e.g. the stop_id: [id1;id2;id3], then after applying to_line, we get [[id1,id2], [id2,id3]]\n",
    ").withColumn(\n",
    "    'stops_name_group', to_line(reduced_df.stop_name)\n",
    "    # same as above\n",
    ").withColumn(\n",
    "    'stops_time_group', to_timetable(reduced_df.arrival_time, reduced_df.departure_time)\n",
    "    # depart at previous stop and arrive at the next stop, applying to_timetable will generate such timetables\n",
    "    # e.g. we have stop_id: [id1;id2;id3], arrival time: [0;1;2], departure_time[0;1.5;2]\n",
    "    # after applying to_timetable, we have [[0,1],[1.5,2]]: depart at stop_1 at 0, arrive at stop_2 at 1; depart at stop_2 at 1.5, arrive at stop_3 at 2.\n",
    ").withColumn(\n",
    "    'used_time', calculate_time(reduced_df.arrival_time, reduced_df.departure_time)\n",
    "    # calculating time spent between stops, from the previous example, we have\n",
    "    # [1, 0.5]\n",
    ").withColumn(\n",
    "    'trans_type', to_transtype(reduced_df.route_desc)\n",
    "    # tranforming the \n",
    ").withColumn(\n",
    "    'route_id', to_transtype(reduced_df.route_id)\n",
    ").select(\n",
    "    'trip_id', 'stops_id_group', 'stops_name_group', 'used_time', 'stops_time_group', 'trans_type', 'route_id'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "26406898-2acf-4581-a5e8-93ff1f2e6f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merge2stops_df = merge2stops_df.withColumn(\n",
    "    \"agg_info\",\n",
    "    combine(merge2stops_df.stops_id_group, merge2stops_df.stops_name_group, merge2stops_df.used_time,\n",
    "            merge2stops_df.stops_time_group, merge2stops_df.trans_type, merge2stops_df.route_id)\n",
    ").withColumn(\n",
    "    \"agg_info\", F.explode(\"agg_info\")\n",
    ").select(\n",
    "    \"trip_id\", F.col(\"agg_info.stops_id_group\").alias(\"stops_id_group\"),\n",
    "    F.col(\"agg_info.stops_name_group\").alias(\"stops_name_group\"),\n",
    "    F.col(\"agg_info.used_time\").alias(\"used_time\"),\n",
    "    F.col(\"agg_info.stops_time_group\").alias(\"stops_time_group\"),\n",
    "    F.col(\"agg_info.trans_type\").alias(\"route_desc\"),\n",
    "    F.col(\"agg_info.route_id\").alias(\"route_id\")\n",
    ")\n",
    "\n",
    "merge2stops_df = merge2stops_df.withColumn(\n",
    "    'new_id_group', remove_parentheses(merge2stops_df.stops_id_group)\n",
    ").withColumn(\n",
    "    'new_name_group', remove_parentheses(merge2stops_df.stops_name_group)\n",
    ").withColumn(\n",
    "    'new_time_group', remove_parentheses(merge2stops_df.stops_time_group)\n",
    ")\n",
    "\n",
    "trans_map = merge2stops_df.select(\n",
    "    \"trip_id\",\n",
    "    F.split(\"new_id_group\", \",\")[0].alias(\"stop_id1\"),\n",
    "    F.split(\"new_id_group\", \", \")[1].alias(\"stop_id2\"),\n",
    "    \"used_time\",\n",
    "    F.split(\"new_time_group\", \",\")[0].alias(\"stop_id1_dep\"),\n",
    "    F.split(\"new_time_group\", \", \")[1].alias(\"stop_id2_arr\"),\n",
    "    \"route_desc\",\n",
    "    \"route_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6a44d25b-7953-4e4e-8d07-e2a44fa99c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------+---------+------------+------------+----------+------------+\n",
      "|             trip_id|   stop_id1|stop_id2|used_time|stop_id1_dep|stop_id2_arr|route_desc|    route_id|\n",
      "+--------------------+-----------+--------+---------+------------+------------+----------+------------+\n",
      "|439.TA.92-655-j22...|8575918:0:A| 8575919|     60.0|    10:23:00|    10:24:00|         B|92-655-j22-1|\n",
      "|439.TA.92-655-j22...|    8575919| 8588311|    120.0|    10:24:00|    10:26:00|         B|92-655-j22-1|\n",
      "|439.TA.92-655-j22...|    8588311| 8575909|     60.0|    10:26:00|    10:27:00|         B|92-655-j22-1|\n",
      "|439.TA.92-655-j22...|    8575909| 8588312|    120.0|    10:27:00|    10:29:00|         B|92-655-j22-1|\n",
      "|439.TA.92-655-j22...|    8588312| 8575946|     60.0|    10:29:00|    10:30:00|         B|92-655-j22-1|\n",
      "+--------------------+-----------+--------+---------+------------+------------+----------+------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "trans_map.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "16ab4fa6-6ec7-498d-9cd6-7cb511c7348b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-------------+---------+-------------+-------------+----------+--------+\n",
      "|trip_id|stop_id_1|    stop_id_2|used_time|stops_id1_dep|stops_id2_arr|route_desc|route_id|\n",
      "+-------+---------+-------------+---------+-------------+-------------+----------+--------+\n",
      "|   walk|  8500926|      8590616|146.91576|         null|         null|      walk|    walk|\n",
      "|   walk|  8500926|      8590737|359.59048|         null|         null|      walk|    walk|\n",
      "|   walk|  8502186|8502186:0:1/2| 9.871648|         null|         null|      walk|    walk|\n",
      "|   walk|  8502186|      8502270| 552.5724|         null|         null|      walk|    walk|\n",
      "|   walk|  8502186|      8590200| 580.6377|         null|         null|      walk|    walk|\n",
      "+-------+---------+-------------+---------+-------------+-------------+----------+--------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "# the walking time we processed earlier\n",
    "walk_map.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e967f862-6cbf-49d1-8e00-bafcf54fa2bd",
   "metadata": {},
   "source": [
    "## UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "187e3047-0f76-4aa7-882f-b39991278562",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".widget-label {\n",
       "    min-width: 150px;\n",
       "    text-align: right;\n",
       "    padding-right: 10px;\n",
       "}\n",
       "#container {\n",
       "    background-color: orange;\n",
       "    color: white;\n",
       "}\n",
       "#title {\n",
       "    color: red;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60bd20001a994a60b9f8473f0be91dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2 id=\"title\">Route Planner</h2>'), Text(value='Küsnacht ZH', description='Departu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%local\n",
    "import pandas as pd\n",
    "from ipywidgets import widgets, interact, VBox\n",
    "from IPython.display import display, HTML\n",
    "import datetime\n",
    "current_datetime = datetime.datetime.now()\n",
    "current_hour = current_datetime.hour\n",
    "current_minute = str(current_datetime.minute)\n",
    "proposed_hour = str(current_hour + 4)\n",
    "\n",
    "def create_schedule(change):\n",
    "    Departure = departure_widget.value\n",
    "    Destination = destination_widget.value\n",
    "    timeInput = input_widget.value\n",
    "    hour, minute = map(int, timeInput.split(\":\"))\n",
    "    time = hour + minute / 60\n",
    "    \n",
    "    schedule_info = {\n",
    "        'dep': [Departure],\n",
    "        'destination': [Destination],\n",
    "        'arrival_time': [time]\n",
    "    }\n",
    "    \n",
    "    schedule_df = pd.DataFrame(schedule_info)\n",
    "    \n",
    "    schedule_df.to_csv('./info.csv')\n",
    "\n",
    "    \n",
    "css = \"\"\"\n",
    ".widget-label {\n",
    "    min-width: 150px;\n",
    "    text-align: right;\n",
    "    padding-right: 10px;\n",
    "}\n",
    "#container {\n",
    "    background-color: orange;\n",
    "    color: white;\n",
    "}\n",
    "#title {\n",
    "    color: red;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "html = \"<style>{}</style>\".format(css)\n",
    "display(HTML(html))\n",
    "    \n",
    "title = widgets.HTML('<h2 id=\"title\">Route Planner</h2>')\n",
    "departure_widget = widgets.Text(value='Küsnacht ZH', description='Departure')\n",
    "destination_widget = widgets.Text(value='Zürich, Neeserweg', description='Destination')\n",
    "input_widget = widgets.Text(value = proposed_hour + \":\"+current_minute, description = 'Arrive at (HH:MM)')\n",
    "\n",
    "input_widget.continuous_update = False\n",
    "input_widget.observe(create_schedule, 'value')\n",
    "\n",
    "container = VBox([title, departure_widget, destination_widget,input_widget], layout=widgets.Layout(id='container'))\n",
    "display(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fffbe18-3efd-4e79-a907-c5859947b997",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Reading input data and sending it to spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d69248de-7b5d-4fae-a277-ece57df7d030",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import pandas as pd\n",
    "schedule_df = pd.read_csv('./info.csv')\n",
    "time_local = schedule_df.iloc[0,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48500e26-cf3c-4cdc-a04a-25eabd23d1e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully passed 'schedule_df' as 'schedule_df' to Spark kernel"
     ]
    }
   ],
   "source": [
    "%%send_to_spark -i schedule_df -t df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9874f2ba-12fa-4f54-aff3-b3a2566a1740",
   "metadata": {},
   "source": [
    "Getting the stop id corresponding to the stop name. \n",
    "\n",
    "Here I preserve only the root stations (no parent stations) and distinct them based on coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d68551c-692c-4a5c-a7ed-88ae5d6c0b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stop_name1 = schedule_df.select('dep').rdd.flatMap(lambda x: x).collect()[0]\n",
    "stop_name2 = schedule_df.select('destination').rdd.flatMap(lambda x: x).collect()[0]\n",
    "time = schedule_df.select('arrival_time').rdd.flatMap(lambda x: x).collect()[0]\n",
    "\n",
    "stop1 = stops_in_15.filter(stops_in_15[\"stop_name\"]==stop_name1)\\\n",
    ".filter(stops_in_15['parent_station'].isNull())\\\n",
    ".dropDuplicates(['stop_lat', 'stop_lon'])\\\n",
    "\n",
    "stop2 = stops_in_15.filter(stops_in_15[\"stop_name\"]==stop_name2)\\\n",
    ".filter(stops_in_15['parent_station'].isNull())\\\n",
    ".dropDuplicates(['stop_lat', 'stop_lon'])\\\n",
    "\n",
    "# the calculation is not stable as I observe, sometimes there are multiple results with\n",
    "# exactly the same coordinates and the only difference is the id, sometimes this does\n",
    "# not happen. Therefore I take the first element of the result.\n",
    "\n",
    "# stop_id1, stop_id2 is a string\n",
    "stop_id1 = stop1.select('stop_id').first().stop_id\n",
    "stop_id2 = stop2.select('stop_id').first().stop_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded4d98e-2a60-4caa-8eb5-a921f6f6ef02",
   "metadata": {},
   "source": [
    "## Graph construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b19a67c-e86d-4870-9bbf-40bb53398f79",
   "metadata": {},
   "source": [
    "Before building the graph, we filter out trips that are:\n",
    "- Arriving later than our desired arrivial time;\n",
    "- Arriving too early (more than 2 hours before) than our desired arrivial time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "848a1431-a636-4930-984a-87e9f7b3913c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "arrival_time_in_hours = F.hour(\"arrival_time\") + F.minute(\"arrival_time\") / 60\n",
    "\n",
    "start_time = time - 2\n",
    "end_time = time\n",
    "\n",
    "trips_arriving_in_time_range = weekday_trans.filter(arrival_time_in_hours.between(start_time, end_time))\n",
    "\n",
    "distinct_trip_ids = trips_arriving_in_time_range.select(\"trip_id\")\n",
    "\n",
    "trans_map = trans_map.join(distinct_trip_ids,'trip_id')\n",
    "\n",
    "trans_map = trans_map.union(walk_map)\n",
    "\n",
    "trans_map.write.parquet('/user/{0}/file/'.format(username), mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "895174a9-89b4-466f-aa5b-860739e1ba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "from hdfs3 import HDFileSystem\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "hdfs = HDFileSystem(host='hdfs://iccluster044.iccluster.epfl.ch', port=8020, user='ebouille')\n",
    "files = hdfs.glob('/user/{0}/file/*.parquet'.format(username))\n",
    "trans = pd.DataFrame()\n",
    "for file in files:\n",
    "    with hdfs.open(file) as f:\n",
    "        trans = pd.concat([trans, pd.read_parquet(f)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7ef2449e-6588-43ba-bac4-d37cc679d5dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "edges = trans.values.tolist()\n",
    "\n",
    "for i, row in enumerate(edges):\n",
    "    edges[i] = (row[1], row[2], {\"time\": float(row[3]), \"trip_id\": row[0], \"route_id\": row[7]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3c390eee-52d2-45cb-9118-19d165cfc363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'used_time': 120.0,\n",
       "  'trip_id': '1139.TA.92-89-j22-1.11.R',\n",
       "  'route_id': '92-89-j22-1'},\n",
       " 1: {'used_time': 168.71622, 'trip_id': 'walk', 'route_id': 'walk'}}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%local\n",
    "g = nx.MultiDiGraph()\n",
    "\n",
    "for edge in edges:\n",
    "    trip_id = edge[2][\"trip_id\"]\n",
    "    route_id = edge[2][\"route_id\"]\n",
    "    stop_id1 = edge[0]\n",
    "    stop_id2 = edge[1]\n",
    "    used_time = edge[2][\"time\"]\n",
    "    if g.has_edge(stop_id1, stop_id2):\n",
    "        edgesNow = g[stop_id1][stop_id2]\n",
    "        same_used_time_edge_exists = any(used_time == edgeNow[\"used_time\"] for edgeNow in edgesNow.values())\n",
    "        if same_used_time_edge_exists:\n",
    "            continue\n",
    "        same_route_id_exists = any(route_id == edgeNow[\"route_id\"] for edgeNow in edgesNow.values())\n",
    "        if same_route_id_exists:\n",
    "            continue\n",
    "    g.add_edge(stop_id1, stop_id2, used_time=used_time, trip_id=trip_id, route_id = route_id)\n",
    "    \n",
    "\n",
    "edge_data = g.get_edge_data('8591365', '8591329')\n",
    "edge_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a708fa-4f1d-4bc8-be55-b5af79f9f3e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

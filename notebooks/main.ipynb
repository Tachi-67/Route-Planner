{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "656ecdf2-1ed8-411e-80e6-9e7bf216aedc",
   "metadata": {},
   "source": [
    "## Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0ee02d-64af-4955-b591-6d7f932ecc4f",
   "metadata": {},
   "source": [
    "We dealt with the following data:\n",
    "\n",
    "- `stop_times` (timetable data)\n",
    "- `calendar` (timetable data)\n",
    "- `routes` (timetable data)\n",
    "- `trips` (timetable data)\n",
    "- `stops` (timetable data)\n",
    "- `actual_condition` (real sbb data)\n",
    "\n",
    "\n",
    "They will be loaded down there. View data structure with `dataName.printSchema()`, to view few (e.g. 5) rows, use `dataName.show(5)`\n",
    "\n",
    "timetable data are on purpose only read from data recorded at 2022/06/01, recall that:\n",
    ">The timetables are updated weekly. It is ok to assume that the weekly changes are small, and a timetable for\n",
    "a given week is thus the same for the full year - use the schedule of the most recent week for the day of the trip.\n",
    "\n",
    "It is also way too expensive to load all data (in fact I tried, the session keeps crushing)\n",
    "\n",
    "---\n",
    "\n",
    "**Proprocessed data:**\n",
    "- `stops_in_15`: Filtered out all stops outside of 15km range; \n",
    "- `walk_map`: Calculated the walking time between walkable stops, date-independent\n",
    "- `weekday_trans`: Combining all timetable data and filtering out weekends, non-business hours; (within 15km range)\n",
    "- `trans_map`: Time that takes from one stop to another for a trip (date-dependent), similar to `walk_map`\n",
    "\n",
    "---\n",
    "\n",
    "**UI:**\n",
    "- Doesn't support fuzzy search, must use exact stop names;\n",
    "- Arrive time input format HH:MM, no spaces.\n",
    "\n",
    "---\n",
    "\n",
    "**Graph:**\n",
    "Directed graph, node is `stop_id`; edge takes two values:\n",
    "- `time`: in seconds, the time it takes from one node to another;\n",
    "- `trip_id`: the id of trip, if is walk, `trip_id` = 'walk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a0e7e92-5438-4fff-9404-bf424641426d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.9/site-packages (3.1)\n",
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.9/site-packages (12.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.9/site-packages (from pyarrow) (1.23.5)\n",
      "Requirement already satisfied: fastparquet in /opt/conda/lib/python3.9/site-packages (2023.4.0)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /opt/conda/lib/python3.9/site-packages (from fastparquet) (1.5.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from fastparquet) (21.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.9/site-packages (from fastparquet) (2023.5.0)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /opt/conda/lib/python3.9/site-packages (from fastparquet) (1.23.5)\n",
      "Requirement already satisfied: cramjam>=2.3 in /opt/conda/lib/python3.9/site-packages (from fastparquet) (2.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas>=1.5.0->fastparquet) (2022.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->fastparquet) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas>=1.5.0->fastparquet) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "#Installing dependencies\n",
    "!pip install networkx\n",
    "!pip install pyarrow\n",
    "!pip install fastparquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b665567f-1764-48b1-82fc-d4aa56344353",
   "metadata": {},
   "source": [
    "## Spark stuff\n",
    "\n",
    "**In peak hours the sesson might not be able to start** and throw:\n",
    "```\n",
    "The code failed because of a fatal error:\n",
    "\tSession xxxx did not start up in 60 seconds..\n",
    "\n",
    "Some things to try:\n",
    "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
    "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
    "c) Restart the kernel.\n",
    "```\n",
    "Solution is to keep retrying ;) good luck for that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53caff7c-0de2-4e15-8fb8-c2340bbe98df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'name': 'haolli-final-project', 'executorMemory': '4G', 'executorCores': 4, 'numExecutors': 10, 'conf': {'spark.jars.repositories': 'https://repos.spark-packages.org'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Error sending http request and maximum retry encountered.\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "import os\n",
    "import json\n",
    "from IPython import get_ipython\n",
    "\n",
    "username = os.environ['RENKU_USERNAME']\n",
    "\n",
    "configuration = dict(\n",
    "    name = f\"{username}-final-project\",\n",
    "    executorMemory = \"4G\",\n",
    "    executorCores = 4,\n",
    "    numExecutors = 10,\n",
    "    conf = {\n",
    "        \"spark.jars.repositories\": \"https://repos.spark-packages.org\",\n",
    "    }\n",
    ")\n",
    "\n",
    "get_ipython().run_cell_magic('configure', line=\"-f\", \n",
    "                             cell=json.dumps(configuration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7dd6dd8-6264-4849-bfab-89ede4b64a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tInvalid status code '400' from http://iccluster044.iccluster.epfl.ch:8998/sessions with error payload: {\"msg\":\"Duplicate session name: Some(haolli-final-project) for session 7618\"}.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "%spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "440d7004-dce0-42df-ac6f-ba31a2020e79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully passed 'username' as 'username' to Spark kernel"
     ]
    }
   ],
   "source": [
    "%%send_to_spark -i username -t str -n username"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5301e5e7-06c7-403a-b5b4-ff05fc629d2f",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4a9e47-7f2d-4e35-8e49-72198334ca9b",
   "metadata": {},
   "source": [
    "### data reading\n",
    "\n",
    "We use data one week of the final presentation but in the year of 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0606136-cdb8-4d34-951f-c995f888380a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "794fa284329546ca8c0ec992704fbd6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we assume little weekly change in the timetable data (see README)\n",
    "# we wish to calculate the connections at the date of 08/06/2023 (day of oral defense)\n",
    "# for the data, there are 5 entries every month. For example, `hdfs dfs -ls /data/sbb/part_orc/timetables/calendar/year=2022/month=6`\n",
    "# will return 5 entries: day =1, 8, 15, 22, 29, each entry corresspond to the according week's data.\n",
    "# For the sake of simplicity we only choose the year of 2022.\n",
    "# Therefore we read data from 2022/6/8\n",
    "orc_file_path = \"/data/sbb/part_orc/timetables\"\n",
    "stop_times = spark.read.orc(orc_file_path + \"/stop_times/year=2022/month=6/day=8\")\n",
    "calendar = spark.read.orc(orc_file_path + \"/calendar/year=2022/month=6/day=8\")\n",
    "routes = spark.read.orc(orc_file_path + \"/routes/year=2022/month=6/day=8\")\n",
    "trips = spark.read.orc(orc_file_path + \"/trips/year=2022/month=6/day=8\")\n",
    "csv_file_path = \"/data/sbb/part_csv/timetables\"\n",
    "stops_csv = spark.read.csv(csv_file_path + \"/stops/year=2022/month=06/day=08\", header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24bddff2-3335-41ea-966b-34201948c012",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date_of_trip: string (nullable = true)\n",
      " |-- Trip_id: string (nullable = true)\n",
      " |-- Operator_id: string (nullable = true)\n",
      " |-- Operator_abk: string (nullable = true)\n",
      " |-- Operator_name: string (nullable = true)\n",
      " |-- Transport_type: string (nullable = true)\n",
      " |-- Train_number(train): string (nullable = true)\n",
      " |-- Service type(train): string (nullable = true)\n",
      " |-- Circulation_id: string (nullable = true)\n",
      " |-- Means_of_transport_text: string (nullable = true)\n",
      " |-- If_additional: string (nullable = true)\n",
      " |-- If_failed: string (nullable = true)\n",
      " |-- Stop_id: string (nullable = true)\n",
      " |-- Stop_name: string (nullable = true)\n",
      " |-- Arrival_time: string (nullable = true)\n",
      " |-- Actual_arrival_time: string (nullable = true)\n",
      " |-- an_prognose_status: string (nullable = true)\n",
      " |-- Departure_time: string (nullable = true)\n",
      " |-- Actual_departure_time: string (nullable = true)\n",
      " |-- ab_prognose_status: string (nullable = true)\n",
      " |-- Not_stop: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "actual_temp = spark.read.load(\"/data/sbb/part_orc/istdaten\", format=\"orc\", sep=\";\", inferSchema=\"true\", header=\"true\")\n",
    "actual_condition = actual_temp.withColumnRenamed(\"betriebstag\", \"Date_of_trip\")\\\n",
    "                .withColumnRenamed(\"fahrt_bezeichner\", \"Trip_id\")\\\n",
    "                .withColumnRenamed(\"betreiber_id\", \"Operator_id\")\\\n",
    "                .withColumnRenamed(\"betreiber_abk\", \"Operator_abk\")\\\n",
    "                .withColumnRenamed(\"betreiber_name\", \"Operator_name\")\\\n",
    "                .withColumnRenamed(\"produkt_id\", \"Transport_type\")\\\n",
    "                .withColumnRenamed(\"linien_id\", \"Train_number(train)\")\\\n",
    "                .withColumnRenamed(\"linien_text\", \"Service type(train)\")\\\n",
    "                .withColumnRenamed(\"umlauf_id\", \"Circulation_id\")\\\n",
    "                .withColumnRenamed(\"verkehrsmittel_text\", \"Means_of_transport_text\")\\\n",
    "                .withColumnRenamed(\"zusatzfahrt_tf\", \"If_additional\")\\\n",
    "                .withColumnRenamed(\"faellt_aus_tf\", \"If_failed\")\\\n",
    "                .withColumnRenamed(\"bpuic\", \"Stop_id\")\\\n",
    "                .withColumnRenamed(\"haltestellen_name\", \"Stop_name\")\\\n",
    "                .withColumnRenamed(\"ankunftszeit\", \"Arrival_time\")\\\n",
    "                .withColumnRenamed(\"an_prognose\", \"Actual_arrival_time\")\\\n",
    "                .withColumnRenamed(\"abfahrtszeit\", \"Departure_time\")\\\n",
    "                .withColumnRenamed(\"ab_prognose\", \"Actual_departure_time\")\\\n",
    "                .withColumnRenamed(\"durchfahrt_tf\", \"Not_stop\")\n",
    "actual_condition.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324f53aa-5aa3-4225-bc25-0486de28d812",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3e62a7-089b-4153-8f49-d1e300148fac",
   "metadata": {},
   "source": [
    "We filter out all stations that are 15kms away from the given Zurich location.\n",
    "\n",
    "For distance calculation, refer to [Haversine formula](https://en.wikipedia.org/wiki/Haversine_formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10ba279d-7026-407e-bad4-eff22b312dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+----------------+----------------+-------------+--------------+\n",
      "|      stop_id|           stop_name|        stop_lat|        stop_lon|location_type|parent_station|\n",
      "+-------------+--------------------+----------------+----------------+-------------+--------------+\n",
      "|      8500926|Oetwil a.d.L., Sc...|47.4236270123012| 8.4031825286317|         null|          null|\n",
      "|      8502186|Dietikon Stoffelbach|47.3933267759652|8.39896044679575|         null| Parent8502186|\n",
      "|8502186:0:1/2|Dietikon Stoffelbach|47.3933997509195|8.39894248049007|         null| Parent8502186|\n",
      "|      8502187|Rudolfstetten Hof...|47.3646702178563|8.37695172233176|         null| Parent8502187|\n",
      "|8502187:0:1/2|Rudolfstetten Hof...|47.3647371479356|8.37703257070734|         null| Parent8502187|\n",
      "+-------------+--------------------+----------------+----------------+-------------+--------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "from math import sin, cos, sqrt, atan2, radians\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "@F.udf(returnType=FloatType())\n",
    "\n",
    "# Filtering out all stops outside of 15km range with the Haversine formula\n",
    "def distance_calculation(latitude_1, longitude_1, latitude_2, longitude_2):\n",
    "    #Use Haversine formula. \n",
    "    #The Haversine formula calculates the distance between two points on a sphere \n",
    "    #(such as the Earth) based on their latitude and longitude.\n",
    "    radius_of_Earth = 6371.0 #Earth radius, just refer to the actual data \n",
    "    \n",
    "    #First, Convert latitude and longitude from degrees to radians\n",
    "    latitude_1 = radians(float(latitude_1))\n",
    "    latitude_2 = radians(float(latitude_2))\n",
    "    longitude_1 = radians(float(longitude_1))\n",
    "    longitude_2 = radians(float(longitude_2))\n",
    "\n",
    "    ## Haversine formula implementation\n",
    "    delta_latitude = latitude_2 - latitude_1\n",
    "    delta_longitude = longitude_2 - longitude_1\n",
    "\n",
    "    a = cos(latitude_1)*cos(latitude_2)*sin(delta_longitude/2)**2+sin(delta_latitude/2)**2\n",
    "    c = 2*atan2(sqrt(a), sqrt(1-a))\n",
    "\n",
    "    distance = radius_of_Earth * c \n",
    "    return distance\n",
    "\n",
    "stops_in_15 = stops_csv.where(distance_calculation(F.lit(47.378177), F.lit(8.540192), F.col(\"stop_lat\"), F.col(\"stop_lon\")) <=15)\n",
    "\n",
    "stops_in_15.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1cc168-a81b-466f-a32a-df10874cb970",
   "metadata": {},
   "source": [
    "With the within-15km stops, we want to preprocess the walking time, for this there are two steps:\n",
    "- Filter out stations that are too far away for walking (>500m)\n",
    "- Calculate walking time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7045bb6f-d6f4-422c-80fc-6014e19c2618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# rename the stops_in_15 dataframe\n",
    "# use crossJoin to make pairs between different stops\n",
    "# calculate the distance between a pair of stops\n",
    "# filtering out stop pairs that are too far away\n",
    "walking_df = stops_in_15.select(F.col(\"stop_id\").alias(\"stop_id_1\"), F.col(\"stop_name\").alias(\"stop_name_1\"), F.col(\"stop_lat\").alias(\"stop_lat_1\"), F.col(\"stop_lon\").alias(\"stop_lon_1\")) \\\n",
    "    .crossJoin(stops_in_15.select(F.col(\"stop_id\").alias(\"stop_id_2\"), F.col(\"stop_name\").alias(\"stop_name_2\"), F.col(\"stop_lat\").alias(\"stop_lat_2\"),F.col(\"stop_lon\").alias(\"stop_lon_2\"))) \\\n",
    "    .withColumn(\"distance\", distance_calculation(F.col(\"stop_lat_1\"), F.col(\"stop_lon_1\"), F.col(\"stop_lat_2\"), F.col(\"stop_lon_2\"))) \\\n",
    "    .select(F.col(\"stop_id_1\"), F.col(\"stop_name_1\"), F.col(\"stop_id_2\"), F.col(\"stop_name_2\"), F.col(\"distance\")) \\\n",
    "    .filter(\"distance<=0.5 and distance>0.0\")\n",
    "# calculating the time spent by walking between the filtered stop pairs\n",
    "walking_df = walking_df.withColumn(\"used_time\", walking_df.distance*1200).select(\"stop_id_1\",\"stop_name_1\",\"stop_id_2\",\"stop_name_2\",\"used_time\")\n",
    "# generating the walk map from the previous result, fill missing values with given default values.\n",
    "walk_map = walking_df.withColumn('trip_id',F.lit('walk')).withColumn('stops_id1_dep',F.lit('null')).withColumn('stops_id2_arr',F.lit('null'))\\\n",
    "                        .withColumn('route_desc', F.lit('walk')).select('trip_id','stop_id_1','stop_id_2','used_time','stops_id1_dep','stops_id2_arr','route_desc').withColumn('route_id', F.lit('walk'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24047ff5-adb1-4503-946c-251ab99d5b04",
   "metadata": {},
   "source": [
    "According to the requirement of the task, we select only the weekdays.\n",
    "\n",
    "The weekdays filtering can be done in calendar and then we use service_id to join other dataframes to get the transportation methods in weekdays.\n",
    "\n",
    "We then implement all the joins using primary keys, and filte out not-business times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3607543-d928-4a5f-b50c-00a617e6a8d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# filtering out week-end data\n",
    "weekdays_mask = calendar.where(\"monday = TRUE and tuesday = TRUE and wednesday = TRUE and thursday = TRUE and friday = TRUE\").select('service_id')\n",
    "weekday_trips = trips.join(weekdays_mask, \"service_id\")\n",
    "# joining all the data together (stops within 15km range)\n",
    "weekday_trips_routes = weekday_trips.join(routes, \"route_id\")\n",
    "weekday_stop_times = stop_times.join(weekday_trips_routes, \"trip_id\")\n",
    "weekday_trans = weekday_stop_times.join(stops_in_15, \"stop_id\")\n",
    "# filtering out non-business hours\n",
    "time_range = (8,18)\n",
    "# selecting only the columns of interest\n",
    "weekday_trans = weekday_trans.filter(F.hour(weekday_trans.arrival_time)>=time_range[0])\\\n",
    "                        .filter(F.hour(weekday_trans.departure_time)>=time_range[0])\\\n",
    "                        .filter(F.hour(weekday_trans.arrival_time)<=time_range[1])\\\n",
    "                        .filter(F.hour(weekday_trans.departure_time)<=time_range[1])\\\n",
    "                        .select(\"trip_id\",\"stop_id\",\"stop_name\",\"arrival_time\",\"departure_time\",\"stop_sequence\", \"route_desc\", \"route_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0507c550-40d0-4ac4-ad92-796139ac01bc",
   "metadata": {},
   "source": [
    "## Graph building\n",
    "\n",
    "Building the edges (time spend between two stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eaf170-7fa0-49ab-8abd-dee9f2241ed4",
   "metadata": {},
   "source": [
    "For one trip (identified by `trip_id`), we aggregate info about it and store it all in one row (per `trip_id`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "24999e60-fa27-4e2c-865a-f956d8e89af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import ArrayType, StringType, StructType, StructField, BooleanType\n",
    "\n",
    "@F.udf(returnType=ArrayType(StringType()))\n",
    "def to_line(column):\n",
    "    set_names = column.split(';')\n",
    "    line_set = []\n",
    "    for i in range(len(set_names) - 1):\n",
    "        line_set.append([set_names[i], set_names[i+1]])\n",
    "    return line_set\n",
    "\n",
    "@F.udf(returnType=ArrayType(StringType()))\n",
    "def to_timetable(arr, dep):\n",
    "    arr_time = arr.split(';')\n",
    "    dep_time = dep.split(';')\n",
    "    line_set = []\n",
    "    for i in range(len(arr_time) - 1):\n",
    "        line_set.append([dep_time[i], arr_time[i+1]])\n",
    "    return line_set\n",
    "\n",
    "@F.udf(returnType=ArrayType(StringType()))\n",
    "def to_transtype(route_desc):\n",
    "    routes_desc = route_desc.split(';')\n",
    "    routes = []\n",
    "    for i in range(len(routes_desc)):\n",
    "        routes.append(routes_desc[i])\n",
    "    return routes\n",
    "\n",
    "@F.udf(returnType=ArrayType(StringType()))\n",
    "def to_routeid(route_id):\n",
    "    routes_id = route_id.split(';')\n",
    "    routes_id = []\n",
    "    for i in range(len(routes_id)):\n",
    "        routes_id.append(routes_id[i])\n",
    "    return routes_id\n",
    "    \n",
    "\n",
    "@F.udf(returnType=ArrayType(FloatType()))\n",
    "def calculate_time(arr, dep):\n",
    "    arr_time = arr.split(';')\n",
    "    dep_time = dep.split(';')\n",
    "    time_set = []\n",
    "    for i in range(len(arr_time) - 1):\n",
    "        time = (datetime.strptime(arr_time[i+1], '%H:%M:%S') - datetime.strptime(dep_time[i], '%H:%M:%S')).total_seconds()\n",
    "        time_set.append(time)\n",
    "    return time_set\n",
    "\n",
    "@F.udf(returnType=StringType())\n",
    "def remove_parentheses(cols):\n",
    "    return cols[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4cc8b6d6-e46b-434d-8801-6ad3dc8f21e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import types as T\n",
    "\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"stops_id_group\", T.StringType()),\n",
    "    T.StructField(\"stops_name_group\", T.StringType()),\n",
    "    T.StructField(\"used_time\", T.StringType()),\n",
    "    T.StructField(\"stops_time_group\", T.StringType()),\n",
    "    T.StructField(\"trans_type\", T.StringType()),\n",
    "    T.StructField(\"route_id\", T.StringType())\n",
    "])\n",
    "\n",
    "# zip_arrays takes 5 equal-length lists, and return a list of elements zipped from them.\n",
    "# see: zip (Python) for an example\n",
    "\n",
    "# @sample:\n",
    "# stops_id_group = [[1,2], [2,3], [3,4]]\n",
    "# stops_name_group = [[A,B], [B,C], [C,D]]\n",
    "# used_time = [10, 5, 8]\n",
    "# stops_time_group = [[8:00, 8:10], [8:15, 8:20], [8:20, 8:28]]\n",
    "# trans_type = [\"Bus\", \"Bus\", \"Bus\"]\n",
    "# route_id = [\"id\", \"id\", \"id\"]\n",
    "# zip_arrays(stops_id_group, stops_name_group, used_time, stops_time_group, trans_type, route_id) returns:\n",
    "# [([1,2], [A,B], 10, [8:00, 8:10], \"Bus\", \"id\"),\n",
    "#  ([2,3], [B,C], 5, [8:15, 8:20], \"bUS\", \"id\"),\n",
    "#  ([3,4], [C,D], 8, [8:20, 8:28], \"Bus\", \"id\")]\n",
    "# each element of the list contains information about travelling in a pair of stops (single-directional).\n",
    "\n",
    "\n",
    "def zip_arrays(stops_id_group, stops_name_group, used_time, stops_time_group, trans_type,route_id):\n",
    "    return list(zip(stops_id_group, stops_name_group, used_time, stops_time_group, trans_type, route_id))\n",
    "\n",
    "# combine wraps the zip_arrays function to be a udf function for spark.\n",
    "combine = F.udf(zip_arrays, returnType=T.ArrayType(schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7ec47b-63e5-47b7-8010-f7337838ddf0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad9831fc1424a7cbe31dad1d008d7b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for one trip, there are multiple stops, they are ordered by the `stop_sequence`\n",
    "# from the weekday_trans data, we first concatenate all data that belongs to one trip with the reduceByKey method,\n",
    "# then within one trip_id, sort by the `stop_sequence`,\n",
    "# then concatenate each attribute within one trip, separated by ';', and make a new columns for each attribute with the map method\n",
    "# finally, name the attributes.\n",
    "reduced_rdd = weekday_trans.rdd.map(\n",
    "    lambda row: (\n",
    "        row[0],\n",
    "        [(row[1], row[2], row[3], row[4], row[5], row[6],row[7])]\n",
    "    )\n",
    ").reduceByKey(\n",
    "    lambda x, y: x + y\n",
    ").map(\n",
    "    lambda row: (\n",
    "        row[0],\n",
    "        sorted(row[1], key=lambda text: int(text[4]))\n",
    "    )\n",
    ").map(\n",
    "    lambda row: (\n",
    "        row[0],\n",
    "        \";\".join([e[0] for e in row[1]]),\n",
    "        \";\".join([e[1] for e in row[1]]),\n",
    "        \";\".join([e[2] for e in row[1]]),\n",
    "        \";\".join([e[3] for e in row[1]]),\n",
    "        \";\".join([e[4] for e in row[1]]),\n",
    "        \";\".join([e[5] for e in row[1]]),\n",
    "        \";\".join([e[6] for e in row[1]]),\n",
    "    )\n",
    ")\n",
    "\n",
    "reduced_df = reduced_rdd.toDF(\n",
    "    [\"trip_id\", \"stop_id\", \"stop_name\", \"arrival_time\", \"departure_time\", \"stop_sequence\", \"route_desc\",\"route_id\"]\n",
    ")\n",
    "# call reduced_df.show(5) to get a feeling of what's there.\n",
    "\n",
    "# for the reduced_df, we have:\n",
    "# trip_id: one single string referring to a trip, the trip is time-dependent\n",
    "# stop_id: a series of string, split by ';', the sequential stop_id along the trip, sorted by the stop_sequence.\n",
    "# stop_name: a series of string, split by ';', the sequential stop_name along the trip, sorted by the stop_sequence.\n",
    "# arrival_time: arrival time at each stop, separated by ';'\n",
    "# departure_time: departure time at each stop, separated by ';'\n",
    "# stop_sequence: the stop sequence, not useful, will be discarded after\n",
    "# route_desc: the same values separated by ';'\n",
    "# route_id: the same route_ids, separated by ';'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5edf8090-d25e-4053-80c5-88808847decf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merge2stops_df = reduced_df.withColumn(\n",
    "    'stops_id_group', to_line(reduced_df.stop_id) \n",
    "    # to_line allows a length-2 window sliding across the stop_id sequence, to make pairs of stops along the trip\n",
    "    # e.g. the stop_id: \"id1;id2;id3\", then after applying to_line, we get [[id1,id2], [id2,id3]]\n",
    ").withColumn(\n",
    "    'stops_name_group', to_line(reduced_df.stop_name)\n",
    "    # same as above\n",
    ").withColumn(\n",
    "    'stops_time_group', to_timetable(reduced_df.arrival_time, reduced_df.departure_time)\n",
    "    # depart at previous stop and arrive at the next stop, applying to_timetable will generate such timetables\n",
    "    # e.g. we have stop_id: \"id1;id2;id3\", arrival time: \"0;1;2\", departure_time\"0;1.5;2\"\n",
    "    # after applying to_timetable, we have [[0,1],[1.5,2]]: depart at stop_1 at 0, arrive at stop_2 at 1; depart at stop_2 at 1.5, arrive at stop_3 at 2.\n",
    ").withColumn(\n",
    "    'used_time', calculate_time(reduced_df.arrival_time, reduced_df.departure_time)\n",
    "    # calculating time spent between stops, from the previous example, we have\n",
    "    # [1, 0.5]\n",
    ").withColumn(\n",
    "    'trans_type', to_transtype(reduced_df.route_desc)\n",
    "    # tranforming the ';' separated string to a list\n",
    "    # \"Bus;Bus;Bus\" -> [\"Bus\", \"Bus\", \"Bus\"]\n",
    ").withColumn(\n",
    "    'route_id', to_transtype(reduced_df.route_id)\n",
    "    # same as above\n",
    ").select(\n",
    "    'trip_id', 'stops_id_group', 'stops_name_group', 'used_time', 'stops_time_group', 'trans_type', 'route_id'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "26406898-2acf-4581-a5e8-93ff1f2e6f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merge2stops_df = merge2stops_df.withColumn(\n",
    "    # for comments of combine, see previous block\n",
    "    \"agg_info\",\n",
    "    combine(merge2stops_df.stops_id_group, merge2stops_df.stops_name_group, merge2stops_df.used_time,\n",
    "            merge2stops_df.stops_time_group, merge2stops_df.trans_type, merge2stops_df.route_id)\n",
    ").withColumn(\n",
    "    # explode will turn every single element in the list to become a row\n",
    "    \"agg_info\", F.explode(\"agg_info\")\n",
    ").select(\n",
    "    # renaming\n",
    "    \"trip_id\",\n",
    "    F.col(\"agg_info.stops_id_group\").alias(\"stops_id_group\"),\n",
    "    F.col(\"agg_info.stops_name_group\").alias(\"stops_name_group\"),\n",
    "    F.col(\"agg_info.used_time\").alias(\"used_time\"),\n",
    "    F.col(\"agg_info.stops_time_group\").alias(\"stops_time_group\"),\n",
    "    F.col(\"agg_info.trans_type\").alias(\"route_desc\"),\n",
    "    F.col(\"agg_info.route_id\").alias(\"route_id\")\n",
    ")\n",
    "\n",
    "merge2stops_df = merge2stops_df.withColumn(\n",
    "    'new_id_group', remove_parentheses(merge2stops_df.stops_id_group)\n",
    ").withColumn(\n",
    "    'new_name_group', remove_parentheses(merge2stops_df.stops_name_group)\n",
    ").withColumn(\n",
    "    'new_time_group', remove_parentheses(merge2stops_df.stops_time_group)\n",
    ")\n",
    "\n",
    "trans_map = merge2stops_df.select(\n",
    "    # spliting the paired information into 2 columns.\n",
    "    \"trip_id\",\n",
    "    F.split(\"new_id_group\", \",\")[0].alias(\"stop_id1\"),\n",
    "    F.split(\"new_id_group\", \", \")[1].alias(\"stop_id2\"),\n",
    "    \"used_time\",\n",
    "    F.split(\"new_time_group\", \",\")[0].alias(\"stop_id1_dep\"),\n",
    "    F.split(\"new_time_group\", \", \")[1].alias(\"stop_id2_arr\"),\n",
    "    \"route_desc\",\n",
    "    \"route_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6a44d25b-7953-4e4e-8d07-e2a44fa99c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------+---------+------------+------------+----------+------------+\n",
      "|             trip_id|   stop_id1|stop_id2|used_time|stop_id1_dep|stop_id2_arr|route_desc|    route_id|\n",
      "+--------------------+-----------+--------+---------+------------+------------+----------+------------+\n",
      "|439.TA.92-655-j22...|8575918:0:A| 8575919|     60.0|    10:23:00|    10:24:00|         B|92-655-j22-1|\n",
      "|439.TA.92-655-j22...|    8575919| 8588311|    120.0|    10:24:00|    10:26:00|         B|92-655-j22-1|\n",
      "|439.TA.92-655-j22...|    8588311| 8575909|     60.0|    10:26:00|    10:27:00|         B|92-655-j22-1|\n",
      "|439.TA.92-655-j22...|    8575909| 8588312|    120.0|    10:27:00|    10:29:00|         B|92-655-j22-1|\n",
      "|439.TA.92-655-j22...|    8588312| 8575946|     60.0|    10:29:00|    10:30:00|         B|92-655-j22-1|\n",
      "+--------------------+-----------+--------+---------+------------+------------+----------+------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "trans_map.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "16ab4fa6-6ec7-498d-9cd6-7cb511c7348b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-------------+---------+-------------+-------------+----------+--------+\n",
      "|trip_id|stop_id_1|    stop_id_2|used_time|stops_id1_dep|stops_id2_arr|route_desc|route_id|\n",
      "+-------+---------+-------------+---------+-------------+-------------+----------+--------+\n",
      "|   walk|  8500926|      8590616|146.91576|         null|         null|      walk|    walk|\n",
      "|   walk|  8500926|      8590737|359.59048|         null|         null|      walk|    walk|\n",
      "|   walk|  8502186|8502186:0:1/2| 9.871648|         null|         null|      walk|    walk|\n",
      "|   walk|  8502186|      8502270| 552.5724|         null|         null|      walk|    walk|\n",
      "|   walk|  8502186|      8590200| 580.6377|         null|         null|      walk|    walk|\n",
      "+-------+---------+-------------+---------+-------------+-------------+----------+--------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "# the walking time we processed earlier\n",
    "walk_map.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e967f862-6cbf-49d1-8e00-bafcf54fa2bd",
   "metadata": {},
   "source": [
    "## UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "187e3047-0f76-4aa7-882f-b39991278562",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".widget-label {\n",
       "    min-width: 150px;\n",
       "    text-align: right;\n",
       "    padding-right: 10px;\n",
       "}\n",
       "#container {\n",
       "    background-color: orange;\n",
       "    color: white;\n",
       "}\n",
       "#title {\n",
       "    color: red;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60bd20001a994a60b9f8473f0be91dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2 id=\"title\">Route Planner</h2>'), Text(value='Küsnacht ZH', description='Departu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%local\n",
    "import pandas as pd\n",
    "from ipywidgets import widgets, interact, VBox\n",
    "from IPython.display import display, HTML\n",
    "import datetime\n",
    "current_datetime = datetime.datetime.now()\n",
    "current_hour = current_datetime.hour\n",
    "current_minute = str(current_datetime.minute)\n",
    "proposed_hour = str(current_hour + 4)\n",
    "\n",
    "def create_schedule(change):\n",
    "    Departure = departure_widget.value\n",
    "    Destination = destination_widget.value\n",
    "    timeInput = input_widget.value\n",
    "    hour, minute = map(int, timeInput.split(\":\"))\n",
    "    time = hour + minute / 60\n",
    "    \n",
    "    schedule_info = {\n",
    "        'dep': [Departure],\n",
    "        'destination': [Destination],\n",
    "        'arrival_time': [time]\n",
    "    }\n",
    "    \n",
    "    schedule_df = pd.DataFrame(schedule_info)\n",
    "    \n",
    "    schedule_df.to_csv('./info.csv')\n",
    "\n",
    "    \n",
    "css = \"\"\"\n",
    ".widget-label {\n",
    "    min-width: 150px;\n",
    "    text-align: right;\n",
    "    padding-right: 10px;\n",
    "}\n",
    "#container {\n",
    "    background-color: orange;\n",
    "    color: white;\n",
    "}\n",
    "#title {\n",
    "    color: red;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "html = \"<style>{}</style>\".format(css)\n",
    "display(HTML(html))\n",
    "    \n",
    "title = widgets.HTML('<h2 id=\"title\">Route Planner</h2>')\n",
    "departure_widget = widgets.Text(value='Küsnacht ZH', description='Departure')\n",
    "destination_widget = widgets.Text(value='Zürich, Neeserweg', description='Destination')\n",
    "input_widget = widgets.Text(value = proposed_hour + \":\"+current_minute, description = 'Arrive at (HH:MM)')\n",
    "\n",
    "input_widget.continuous_update = False\n",
    "input_widget.observe(create_schedule, 'value')\n",
    "\n",
    "container = VBox([title, departure_widget, destination_widget,input_widget], layout=widgets.Layout(id='container'))\n",
    "display(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fffbe18-3efd-4e79-a907-c5859947b997",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Reading input data and sending it to spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d69248de-7b5d-4fae-a277-ece57df7d030",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "schedule_df = pd.read_csv('./info.csv')\n",
    "time_local = schedule_df.iloc[0,3]\n",
    "time_local = str(timedelta(hours=time_local))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48500e26-cf3c-4cdc-a04a-25eabd23d1e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully passed 'schedule_df' as 'schedule_df' to Spark kernel"
     ]
    }
   ],
   "source": [
    "%%send_to_spark -i schedule_df -t df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9874f2ba-12fa-4f54-aff3-b3a2566a1740",
   "metadata": {},
   "source": [
    "Getting the stop id corresponding to the stop name. \n",
    "\n",
    "Here I preserve only the root stations (no parent stations) and distinct them based on coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d68551c-692c-4a5c-a7ed-88ae5d6c0b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark -o stop_ids\n",
    "stop_name1 = schedule_df.select('dep').rdd.flatMap(lambda x: x).collect()[0]\n",
    "stop_name2 = schedule_df.select('destination').rdd.flatMap(lambda x: x).collect()[0]\n",
    "time = schedule_df.select('arrival_time').rdd.flatMap(lambda x: x).collect()[0]\n",
    "\n",
    "stop1 = stops_in_15.filter(stops_in_15[\"stop_name\"]==stop_name1)\\\n",
    ".filter(stops_in_15['parent_station'].isNull())\\\n",
    ".dropDuplicates(['stop_lat', 'stop_lon'])\\\n",
    "\n",
    "stop2 = stops_in_15.filter(stops_in_15[\"stop_name\"]==stop_name2)\\\n",
    ".filter(stops_in_15['parent_station'].isNull())\\\n",
    ".dropDuplicates(['stop_lat', 'stop_lon'])\\\n",
    "\n",
    "# the calculation is not stable as I observe, sometimes there are multiple results with\n",
    "# exactly the same coordinates and the only difference is the id, sometimes this does\n",
    "# not happen. Therefore I take the first element of the result.\n",
    "\n",
    "# stop_id1, stop_id2 is a string\n",
    "stop_id1 = stop1.select('stop_id').first().stop_id\n",
    "stop_id2 = stop2.select('stop_id').first().stop_id\n",
    "schema = StructType([\n",
    "    StructField(\"stop_id1\", StringType(), nullable=False),\n",
    "    StructField(\"stop_id2\", StringType(), nullable=False)\n",
    "])\n",
    "stop_ids = spark.createDataFrame([], schema)\n",
    "stop_ids = stop_ids.union(spark.createDataFrame([(stop_id1, stop_id2)], schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c0febf-eace-4bfc-a709-58323d373cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "stop_id1 = stop_ids['stop_id1'].iloc[0]\n",
    "stop_id2 = stop_ids['stop_id2'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded4d98e-2a60-4caa-8eb5-a921f6f6ef02",
   "metadata": {},
   "source": [
    "## Graph construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b19a67c-e86d-4870-9bbf-40bb53398f79",
   "metadata": {},
   "source": [
    "Before building the graph, we filter out trips that are:\n",
    "- Arriving later than our desired arrivial time;\n",
    "- Arriving too early (more than 2 hours before) than our desired arrivial time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "848a1431-a636-4930-984a-87e9f7b3913c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "arrival_time_in_hours = F.hour(\"arrival_time\") + F.minute(\"arrival_time\") / 60\n",
    "\n",
    "start_time = time - 2\n",
    "end_time = time\n",
    "\n",
    "trips_arriving_in_time_range = weekday_trans.filter(arrival_time_in_hours.between(start_time, end_time))\n",
    "\n",
    "distinct_trip_ids = trips_arriving_in_time_range.select(\"trip_id\")\n",
    "\n",
    "trans_map2 = trans_map.join(distinct_trip_ids,'trip_id')\n",
    "\n",
    "trans_map2 = trans_map2.union(walk_map)\n",
    "\n",
    "trans_map2.write.parquet('/user/{0}/file/'.format(username), mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac92a87d-135f-4eb1-b9f0-58d341d5ce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract route_id from trip_id\n",
    "@F.udf(returnType=StringType())\n",
    "def split_trip(t):\n",
    "    tmp = t.split('.')\n",
    "    if len(tmp) > 1: # train/bus/...\n",
    "        return tmp[2]\n",
    "    else: # 'walk'\n",
    "        return t\n",
    "trans_map3 = trans_map2.withColumn('route_id', split_trip(trans_map.trip_id))#.withColumnRenamed(\"trip_id\", \"route_id\")\n",
    "trans_map3.write.mode(\"overwrite\").parquet('/user/{0}/trans_map3/'.format(username))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "895174a9-89b4-466f-aa5b-860739e1ba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "from hdfs3 import HDFileSystem\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "hdfs = HDFileSystem(host='hdfs://iccluster044.iccluster.epfl.ch', port=8020, user='ebouille')\n",
    "files = hdfs.glob('/user/{0}/trans_map3/*.parquet'.format(username))\n",
    "trans = pd.DataFrame()\n",
    "for file in files:\n",
    "    with hdfs.open(file) as f:\n",
    "        trans = pd.concat([trans, pd.read_parquet(f)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7ef2449e-6588-43ba-bac4-d37cc679d5dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "edges = trans.values.tolist()\n",
    "\n",
    "for i, row in enumerate(edges):\n",
    "    edges[i] = (row[1], row[2], {\"time\": float(row[3]), \"trip_id\": row[0], \"route_id\": row[7]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3c390eee-52d2-45cb-9118-19d165cfc363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'used_time': 120.0,\n",
       "  'trip_id': '1139.TA.92-89-j22-1.11.R',\n",
       "  'route_id': '92-89-j22-1'},\n",
       " 1: {'used_time': 168.71622, 'trip_id': 'walk', 'route_id': 'walk'}}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%local\n",
    "from functions import *\n",
    "g = create_multigraph(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a708fa-4f1d-4bc8-be55-b5af79f9f3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local \n",
    "source = stop_id1#'8591365'\n",
    "target = stop_id2#'8591070'\n",
    "paths, route_id_lists = k_shortest_paths(g, source, target, k=10, weight='used_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c4d15a-5ef1-47ba-9743-7220a6a3830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, date_format\n",
    "actual_data_temp = spark.read.load(\"/data/sbb/part_orc/istdaten\", format=\"orc\", sep=\";\", inferSchema=\"true\", header=\"true\")\n",
    "all_data = actual_data_temp.withColumnRenamed(\"betriebstag\", \"date_of_trip\")\\\n",
    "                .withColumnRenamed(\"fahrt_bezeichner\", \"journey_id\")\\\n",
    "                .withColumnRenamed(\"betreiber_id\", \"operator_id\")\\\n",
    "                .withColumnRenamed(\"betreiber_abk\", \"operator_abk\")\\\n",
    "                .withColumnRenamed(\"betreiber_name\", \"operator_name\")\\\n",
    "                .withColumnRenamed(\"produkt_id\", \"transport_type\")\\\n",
    "                .withColumnRenamed(\"linien_id\", \"service_number\")\\\n",
    "                .withColumnRenamed(\"linien_text\", \"service_type\")\\\n",
    "                .withColumnRenamed(\"umlauf_id\", \"circulation_id\")\\\n",
    "                .withColumnRenamed(\"verkehrsmittel_text\", \"means_of_transport_text\")\\\n",
    "                .withColumnRenamed(\"zusatzfahrt_tf\", \"is_additional\")\\\n",
    "                .withColumnRenamed(\"faellt_aus_tf\", \"is_failed\")\\\n",
    "                .withColumnRenamed(\"bpuic\", \"stop_id\")\\\n",
    "                .withColumnRenamed(\"haltestellen_name\", \"stop_name\")\\\n",
    "                .withColumnRenamed(\"ankunftszeit\", \"schedule_arrival_timestamp\")\\\n",
    "                .withColumnRenamed(\"an_prognose\", \"actual_arrival_timestamp\")\\\n",
    "                .withColumnRenamed(\"abfahrtszeit\", \"schedule_departure_timestamp\")\\\n",
    "                .withColumnRenamed(\"ab_prognose\", \"actual_departure_timestamp\")\\\n",
    "                .withColumnRenamed(\"durchfahrt_tf\", \"not_stop\")\\\n",
    "                .withColumn(\"schedule_departure_time\", date_format(to_timestamp(\"schedule_departure_timestamp\", \"dd.MM.yyyy HH:mm\"), \"HH:mm:ss\"))\\\n",
    "                .withColumn(\"schedule_arrival_time\", date_format(to_timestamp(\"schedule_arrival_timestamp\", \"dd.MM.yyyy HH:mm\"), \"HH:mm:ss\"))\\\n",
    "                .withColumn(\"date\", date_format(to_timestamp(\"schedule_departure_timestamp\", \"dd.MM.yyyy HH:mm\"), \"dd.MM.yyyy\"))\n",
    "all_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f845361e-c8fc-449d-912c-8a01bb76bbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, round, unix_timestamp\n",
    "from pyspark.sql.types import DateType\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_probability_of_train_being_on_time(all_data, \n",
    "                                           start_stop_id, \n",
    "                                           end_stop_id,\n",
    "                                           schedule_departure_time, \n",
    "                                           schedule_arrival_time, \n",
    "                                           transport_type, \n",
    "                                           date: str,\n",
    "                                           stopover_time_in_minutes: int):\n",
    "    \n",
    "    # date format\n",
    "    date_format = '%d.%m.%Y'\n",
    "    \n",
    "    # Convert date string to DateType\n",
    "    trip_date = datetime.strptime(str(date), date_format).date()\n",
    "\n",
    "    # Calculate the date 30 days ago\n",
    "    history_lower_bound_date = trip_date - timedelta(days=30)\n",
    "\n",
    "    # Filter the DataFrame to get only the records that were arriving at the destination stop at the same time.\n",
    "    trip_arrivals = all_data.filter(\n",
    "        (col('stop_id') == end_stop_id) &\n",
    "        (col('schedule_arrival_time') == schedule_arrival_time) &\n",
    "        (col('transport_type') == transport_type) &\n",
    "        (col('date').between(trip_date.strftime(date_format), history_lower_bound_date.strftime(date_format)))\n",
    "    )\n",
    "\n",
    "    # Filter the DataFrame to get only the records that were starting at the start stop at the same departure time.\n",
    "    trip_departures = all_data.filter(\n",
    "        (col('stop_id') == start_stop_id) &\n",
    "        (col('schedule_departure_time') == schedule_departure_time) &\n",
    "        (col('transport_type') == transport_type) &\n",
    "        (col('date').between(trip_date.strftime(date_format), history_lower_bound_date.strftime(date_format)))\n",
    "    )\n",
    "\n",
    "    # in the trip arrivals, keep only the records for which the journey_id is also in the trip_departures. \n",
    "    # This is to ensure that the transports taken into consideration are only the trips corresponding to the same lines as we are considering.\n",
    "    trip_arrivals_filtered = trip_arrivals.filter(col('journey_id').isin(trip_departures.select('journey_id').rdd.flatMap(lambda x: x).collect()))\n",
    "\n",
    "    # in the trip_arrivals_filtered, add a column with the computation of the difference \n",
    "    # between actual_arrival_timestamp and schedule_arrival_timestamp so that we could have, \n",
    "    # rounded in minutes, the number of minutes of delay for every trip.\n",
    "    \n",
    "    # Convert timestamp columns to Unix timestamp\n",
    "    trip_arrivals_filtered = trip_arrivals_filtered.withColumn('actual_arrival_unix', unix_timestamp(col('actual_arrival_timestamp'), 'dd.MM.yyyy HH:mm:ss'))\n",
    "    trip_arrivals_filtered = trip_arrivals_filtered.withColumn('schedule_arrival_unix', unix_timestamp(col('schedule_arrival_timestamp'), 'dd.MM.yyyy HH:mm'))\n",
    "\n",
    "    # Calculate the time difference in minutes and round to integers\n",
    "    trip_arrivals_filtered = trip_arrivals_filtered.withColumn('delay_minutes', round((col('actual_arrival_unix') - col('schedule_arrival_unix')) / 60).cast('integer'))\n",
    "\n",
    "    # Calculate the probability of the train being on time\n",
    "    on_time_trips = trip_arrivals_filtered.filter(col('delay_minutes') <= stopover_time_in_minutes).count()\n",
    "    total_trips = trip_arrivals_filtered.count()\n",
    "    \n",
    "    if total_trips == 0:\n",
    "        print(\"default proba = 1 as no trips in history\")\n",
    "        return 1\n",
    "    \n",
    "    probability_of_on_time = on_time_trips / total_trips\n",
    "    \n",
    "    print(\"on_time_trips\", on_time_trips)\n",
    "    print(\"total_trips\", total_trips)\n",
    "    print(\"probability_of_on_time\", probability_of_on_time)\n",
    "    \n",
    "    return probability_of_on_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb8ee31-e1ec-42dd-ad1e-051ad5cf769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "from transport_mapping import transport_type_dict\n",
    "arr_time = time_local\n",
    "date = '13.02.2022'\n",
    "input_dfs = []\n",
    "trip_durations = []\n",
    "for i, (stops, route_ids) in enumerate(zip(paths, route_id_lists)):\n",
    "    \n",
    "    path_stops = find_stops(stops)\n",
    "    path_stops['route_id'] = path_stops.trip_id.apply(lambda r: r.split('.')[2])\n",
    "    latest_arrivals = calculate_connections(trans,path_stops,route_ids,stops,arr_time)\n",
    "    \n",
    "    link = get_connection_info(trans,latest_arrivals, stops, date, arr_time)\n",
    "    \n",
    "    input_df = pd.DataFrame(link, columns=['stop_id1','stop_id2','departure_time','arrival_time',\n",
    "                                                  'transport_type','date','stopover_time_in_minutes', 'route_id'])\n",
    "    input_df.date = [date]*len(input_df.date)\n",
    "    input_df.transport_type = input_df.transport_type.apply(lambda r: transport_type_dict[r])\n",
    "    input_df['index'] = [i]*len(input_df.date)\n",
    "    input_dfs.append(input_df)\n",
    "    trip_durations.append(calculate_total_time(input_df))\n",
    "input_dfs = pd.concat(input_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5381396d-2f80-4424-82eb-43b6541c2fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%send_to_spark -i input_dfs -t df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8ef4e6-e104-4199-b027-92e2995e5426",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -o probs\n",
    "path_ids = input_dfs.select('index').distinct().collect()\n",
    "probs = []\n",
    "for path_id in path_ids:\n",
    "    prob = 1.0\n",
    "    for r in input_dfs.filter(col('index') == path_id['index']).collect():\n",
    "        prob *= get_probability_of_train_being_on_time(all_data, r['stop_id1'], r['stop_id2'], \n",
    "                                                  r['departure_time'], r['arrival_time'],\n",
    "                                                  r['transport_type'], r['date'], r['stopover_time_in_minutes'])\n",
    "    probs.append(prob)\n",
    "schema = T.StructType([T.StructField('index', T.IntegerType()), T.StructField('probs', T.FloatType())])\n",
    "probs = spark.createDataFrame(data=[(path_id['index'], p) for path_id, p in zip(path_ids, probs)], schema=schema)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
